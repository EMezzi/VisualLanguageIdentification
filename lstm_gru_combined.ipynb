{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLtJFjYdEdjx"
   },
   "source": [
    "# **Dataset bilanciato**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eKRVjRGjcDFO",
    "outputId": "6f7fa4a2-4852-4f68-d4c8-8fdf3d2f9a89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Montaggio del drive, per poi ricavare i file binari\n",
    "\"\"\"\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fO2YE5zAwcco"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Questa è l'operazione di caricamento. Per eseguirla è necessario scegliere \n",
    "una cartella su drive dove inserire i file binari\n",
    "\"\"\"\n",
    "\n",
    "import dill \n",
    "\n",
    "x_test_12_0 = dill.load(open(\"/content/drive/MyDrive/Land_12/balanced/x_test_12_0\", \"rb\"))\n",
    "y_test_12 = dill.load(open(\"/content/drive/MyDrive/Land_12/balanced/y_test_12\", \"rb\"))\n",
    "\n",
    "x_train_12_0 = dill.load(open(\"/content/drive/MyDrive/Land_12/balanced/x_train_12_0\", \"rb\"))\n",
    "y_train_12 = dill.load(open(\"/content/drive/MyDrive/Land_12/balanced/y_train_12\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzeGwSRWCQ0A"
   },
   "source": [
    "# **LSTM e GRU combinati, modello unidirezionale**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2VXuLSqwkry"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import dill\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten, GRU, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "path = \"/content/drive/MyDrive/lstm_gru_uni_best\"\n",
    "callback_check = ModelCheckpoint(path, monitor=\"val_accuracy\", \n",
    "                                 save_best_only=True)\n",
    "callbacks = [callback_check]\n",
    "initializer_gru = tf.keras.initializers.Constant(value=1)\n",
    "\n",
    "dict_weights = {0: .50, 1: .50, 2: .50, 3: .25, 4: 1., 5: 1., 6: 0.5}\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128, input_shape=(x_train_12_0.shape[1:]), return_sequences=True, \n",
    "               activation=\"elu\"))\n",
    "model.add(GRU(128, return_sequences=True, activation=\"elu\"))\n",
    "model.add(LSTM(128, return_sequences=True, activation=\"elu\"))\n",
    "model.add(GRU(128, return_sequences=True, activation=\"elu\"))\n",
    "model.add(LSTM(128, return_sequences=False, activation=\"elu\"))\n",
    "\"\"\"\n",
    "model.add(GRU(128, return_sequences=True, bias_initializer=initializer_gru, \n",
    "              kernel_initializer=\"glorot_normal\"))\n",
    "model.add(LSTM(128, return_sequences=True, bias_initializer=\"zeros\", \n",
    "              kernel_initializer=\"glorot_normal\"))\n",
    "model.add(GRU(128, return_sequences=True, bias_initializer=initializer_gru,\n",
    "              kernel_initializer=\"glorot_normal\"))\n",
    "model.add(LSTM(128, return_sequences=True, bias_initializer=\"zeros\", \n",
    "               kernel_initializer=\"glorot_normal\"))\n",
    "model.add(GRU(128, return_sequences=True, bias_initializer=initializer_gru, \n",
    "              kernel_initializer=\"glorot_normal\"))\n",
    "model.add(LSTM(128, bias_initializer=\"zeros\", \n",
    "              kernel_initializer=\"glorot_normal\"))\n",
    "\"\"\"\n",
    "\n",
    "model.add(Dense(64, activation=\"elu\"))\n",
    "model.add(Dense(7, activation=\"softplus\"))\n",
    "\n",
    "opt = tf.optimizers.Adamax(0.0002)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train_12_0, y_train_12, batch_size=32, shuffle=True, epochs=800, \n",
    "          callbacks=callbacks, validation_data=(x_test_12_0, y_test_12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGYVrqt1EuZp"
   },
   "source": [
    "# **Tre strati LSTM bidirezionali**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHpr8Tu3N0V6"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as b\n",
    "\n",
    "def modified_elliot(x_tensor):\n",
    "  x_tensor = (x_tensor / b.sqrt(1 + b.pow(x_tensor, 2))) + 0.5\n",
    "  return x_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HU5HGQQIwq4a"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import dill\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, GRU, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "path = \"/content/drive/MyDrive/bid_best\"\n",
    "callback_check = ModelCheckpoint(path, monitor=\"val_accuracy\", \n",
    "                                 save_best_only=True)\n",
    "callbacks = [callback_check]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Bidirectional(LSTM(66, input_shape=(x_train_12_0.shape[1:]), \n",
    "                             return_sequences=True, \n",
    "                             bias_initializer=\"glorot_uniform\")))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
    "\n",
    "model.add(Dense(32, bias_initializer=\"random_uniform\"))\n",
    "model.add(Dense(7, activation=\"softplus\"))\n",
    "\n",
    "opt = tf.optimizers.Adadelta(learning_rate=0.001, rho=0.95)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train_12_0, y_train_12, batch_size=16, shuffle=True, epochs=400, \n",
    "          callbacks=callbacks, validation_data=(x_test_12_0, y_test_12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1oeQ7qIzMNO"
   },
   "source": [
    "# **Rete con LSTM e GRU alternati con Modified Elliot e Orthogonal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvM2VTF51L62"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "initializer_gru = tf.keras.initializers.Constant(value=-1)\n",
    "initializer_lstm = tf.keras.initializers.Constant(value=1)\n",
    "\n",
    "path = \"/content/drive/MyDrive/lstm_gru_elliot_orthogonal\"\n",
    "callback_check = ModelCheckpoint(path, monitor=\"val_accuracy\", \n",
    "                                 save_best_only=True)\n",
    "callbacks = [callback_check]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128, input_shape=(x_train_12_0.shape[1:]), return_sequences=True, \n",
    "               bias_initializer=\"zeros\", \n",
    "               kernel_initializer=\"glorot_uniform\", \n",
    "               recurrent_activation=modified_elliot))\n",
    "\n",
    "model.add(GRU(128, return_sequences=True, \n",
    "              bias_initializer=initializer_gru, \n",
    "              kernel_initializer=\"glorot_uniform\", \n",
    "              recurrent_activation=modified_elliot))\n",
    "\n",
    "model.add(LSTM(128, bias_initializer=\"zeros\", \n",
    "               kernel_initializer=\"glorot_uniform\", \n",
    "               recurrent_activation=modified_elliot))\n",
    "\n",
    "model.add(Dense(32, bias_initializer=\"glorot_uniform\"))\n",
    "model.add(Dense(7, activation=\"softmax\"))\n",
    "\n",
    "opt = tf.optimizers.Adam(0.0002)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train_12_0, y_train_12, batch_size=32, shuffle=True, epochs=100, \n",
    "          callbacks=callbacks, validation_data=(x_test_12_0, y_test_12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlKecmk2VzRM"
   },
   "source": [
    "# **Modello unidirezionale per dataset sbilanciato applicato a dataset bilanciato**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XLt3lCRV8XO"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "path = \"/content/drive/MyDrive/uni_best\"\n",
    "callback_check = ModelCheckpoint(path, monitor=\"val_accuracy\", \n",
    "                                 save_best_only=True)\n",
    "callbacks = [callback_check]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128, input_shape=(x_train_12_0.shape[1:]), return_sequences=True, \n",
    "               kernel_initializer=\"glorot_normal\", dropout=0.5))\n",
    "model.add(GRU(128, return_sequences=True, dropout=0.5))\n",
    "model.add(LSTM(128, return_sequences=True, dropout=0.5))\n",
    "model.add(GRU(128, return_sequences=True, dropout=0.5))\n",
    "model.add(LSTM(128, return_sequences=True, dropout=0.5))\n",
    "model.add(GRU(128, return_sequences=True, dropout=0.5))\n",
    "model.add(LSTM(128, dropout=0.5))\n",
    "\n",
    "model.add(Dense(32, bias_initializer=\"glorot_uniform\"))\n",
    "model.add(Dense(7, activation=\"softmax\"))\n",
    "\n",
    "opt = tf.optimizers.Adamax(0.0002)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train_12_0, y_train_12, batch_size=32, shuffle=True, epochs=4000, \n",
    "          callbacks=callbacks, validation_data=(x_test_12_0, y_test_12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmUDNMdUGW3u"
   },
   "source": [
    "# **Modello ConvLSTM2D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtBl03PIGet6"
   },
   "outputs": [],
   "source": [
    "import dill \n",
    "\n",
    "x_test_conv = dill.load(open(\"/content/drive/MyDrive/Convolutional/without_nose/x_test_0\", \"rb\"))\n",
    "y_test_conv = dill.load(open(\"/content/drive/MyDrive/Convolutional/without_nose/y_test\", \"rb\"))\n",
    "\n",
    "x_train_conv = dill.load(open(\"/content/drive/MyDrive/Convolutional/without_nose/x_train_0\", \"rb\"))\n",
    "y_train_conv = dill.load(open(\"/content/drive/MyDrive/Convolutional/without_nose/y_train\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3tBviUkv62u",
    "outputId": "3205e88c-d7dc-430c-cb8a-82b7717ea6e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 150, 25, 35, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_conv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rv3sXZ_NmpkI"
   },
   "outputs": [],
   "source": [
    "def create_samples(x, y): \n",
    "  samples = []\n",
    "  for i in range(0, len(x)): \n",
    "    samples.append((x[i], y[i]))\n",
    "  \n",
    "  return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogZ2OHyP4hjB"
   },
   "outputs": [],
   "source": [
    "samples_train = create_samples(x_train_conv, y_train_conv)\n",
    "samples_test = create_samples(x_test_conv, y_test_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61J66DdIkbRw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generator_(samples, batch_size):\n",
    "\n",
    "  num_samples = len(samples)\n",
    "  while True:\n",
    "    for offset in range(0, num_samples, batch_size): \n",
    "\n",
    "      batch_samples = samples[offset:offset+batch_size]\n",
    "      batch_target = samples[offset:offset+batch_size]\n",
    "\n",
    "      x_train = []\n",
    "      y_train = []\n",
    "\n",
    "      for i in range(0, len(batch_samples)): \n",
    "        x_train.append(batch_samples[i][0])\n",
    "        y_train.append(batch_target[i][1])\n",
    "\n",
    "      x_train = np.array(x_train)\n",
    "      y_train = np.array(y_train)\n",
    "      \n",
    "      yield x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlPMIlT0iMh7"
   },
   "outputs": [],
   "source": [
    "train_generator = generator_(samples_train, 4)\n",
    "test_generator = generator_(samples_test, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13gTcIcHHoKB"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Dense, Dropout, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(ConvLSTM2D(64, kernel_size=(3, 3), return_sequences=True, \n",
    "                     data_format=\"channels_last\", \n",
    "                     input_shape=x_train_conv.shape[1:]))\n",
    "model.add(ConvLSTM2D(64, kernel_size=(3, 3), return_sequences=True, dropout=0.5))\n",
    "model.add(ConvLSTM2D(64, kernel_size=(3, 3), return_sequences=True, dropout=0.5))\n",
    "model.add(ConvLSTM2D(64, kernel_size=(3, 3), return_sequences=False))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(7, activation=\"softmax\"))\n",
    "\n",
    "opt = tf.keras.optimizers.Adamax(0.0002)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, \n",
    "              metrics=\"accuracy\")\n",
    "\n",
    "model.fit(train_generator, shuffle=True, epochs=50, steps_per_epoch=21, \n",
    "          validation_data=(x_test_conv, y_test_conv))\n",
    "\n",
    "model.evaluate(x_test_conv, y_test_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKnPdKSl0Wpj"
   },
   "source": [
    "# **Dataset 8 landmark bilanciato con 7 lingue**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-E2Vnd9D0V7G"
   },
   "outputs": [],
   "source": [
    "import dill \n",
    "\n",
    "x_test_8_0 = dill.load(open(\"/content/drive/MyDrive/Land_8/balanced_7_languages/x_test_8_0\", \"rb\"))\n",
    "y_test_8 = dill.load(open(\"/content/drive/MyDrive/Land_8/balanced_7_languages/y_test_8\", \"rb\"))\n",
    "\n",
    "x_train_8_0 = dill.load(open(\"/content/drive/MyDrive/Land_8/balanced_7_languages/x_train_8_0\", \"rb\"))\n",
    "y_train_8 = dill.load(open(\"/content/drive/MyDrive/Land_8/balanced_7_languages/y_train_8\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAB_uuio1JBQ",
    "outputId": "f4478308-0add-486e-ef97-4c5444f0521f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(531, 330, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_8_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDV7zv-xvE-c"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "path = \"/content/drive/MyDrive/uni_balanced_best_8_7\"\n",
    "callback_check = ModelCheckpoint(path, monitor=\"val_accuracy\", \n",
    "                                 save_best_only=True)\n",
    "callbacks = [callback_check]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128, input_shape=(x_train_8_0.shape[1:]), \n",
    "                return_sequences=True, \n",
    "                bias_initializer=\"glorot_uniform\"))\n",
    "model.add(GRU(128, return_sequences=True))\n",
    "model.add(LSTM(128, return_sequences=True, activity_regularizer=regularizers.l2(1e-5)))\n",
    "model.add(GRU(128, return_sequences=True))\n",
    "model.add(LSTM(128, return_sequences=True, activity_regularizer=regularizers.l2(1e-5)))\n",
    "model.add(GRU(128, return_sequences=True))\n",
    "model.add(LSTM(128, return_sequences=True, activity_regularizer=regularizers.l2(1e-5)))\n",
    "model.add(GRU(128, return_sequences=True))\n",
    "model.add(LSTM(64, return_sequences=True, activity_regularizer=regularizers.l2(1e-5)))\n",
    "model.add(GRU(64, return_sequences=True))\n",
    "model.add(LSTM(64, return_sequences=True, activity_regularizer=regularizers.l2(1e-5)))\n",
    "model.add(GRU(64, return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "\n",
    "model.add(Dense(32, bias_initializer=\"glorot_uniform\"))\n",
    "model.add(Dense(7, activation=\"softmax\"))\n",
    "\n",
    "opt = tf.keras.optimizers.Adamax(0.002)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train_8_0, y_train_8, batch_size=16, shuffle=True, epochs=400, \n",
    "          callbacks=callbacks, validation_data=(x_test_8_0, y_test_8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwLhtYcEZg3N"
   },
   "source": [
    "# **Modello bidirezionale**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-CBlL_UZPnT"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import dill\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, GRU, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "path = \"/content/drive/MyDrive/bid_balanced_best_8_7\"\n",
    "callback_check = ModelCheckpoint(path, monitor=\"val_accuracy\", \n",
    "                                 save_best_only=True)\n",
    "callbacks = [callback_check]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Bidirectional(LSTM(128, input_shape=(x_train_12_0.shape[1:]), \n",
    "                             return_sequences=True, \n",
    "                             bias_initializer=\"glorot_uniform\")))\n",
    "model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "\n",
    "model.add(Dense(32, bias_initializer=\"random_uniform\"))\n",
    "model.add(Dense(7, activation=\"sofmax\"))\n",
    "\n",
    "opt = tf.optimizers.Adagrad(learning_rate=0.001)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train_8_0, y_train_8, batch_size=32, shuffle=True, epochs=400, \n",
    "          callbacks=callbacks, validation_data=(x_test_8_0, y_test_8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OK6_NqDCBCL"
   },
   "source": [
    "# **Valutazione del modello**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9A-q5UcqjkTu"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "\n",
    "model_one = models.load_model(\"/content/drive/MyDrive/bid_balanced_best_8_7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z34KkRKkC8a1"
   },
   "outputs": [],
   "source": [
    "model_one.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQz5DMtCj7Oj"
   },
   "outputs": [],
   "source": [
    "label = [\"italiano\", \"inglese\", \"tedesco\", \"spagnolo\", \"olandese\", \"russo\", \n",
    "         \"giapponese\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gNegG04ZDo8u",
    "outputId": "295859e9-23da-4958-9b7e-b1e18f4d5f51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 85ms/step - loss: 1.6959 - accuracy: 0.4595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6958529949188232, 0.45945945382118225]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_one.evaluate(x_test_8_0, y_test_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlLL6ydcj-VY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predictions = model_one.predict(x_test_8_0)\n",
    "max_predictions = []\n",
    "\n",
    "corrette_lingua = {\"italiano\": 0, \"inglese\": 0, \"tedesco\": 0, \"spagnolo\": 0, \n",
    "        \"olandese\": 0, \"russo\": 0, \"giapponese\": 0}\n",
    "totali_lingua = {\"italiano\": 0, \"inglese\": 0, \"tedesco\": 0, \"spagnolo\": 0, \n",
    "        \"olandese\": 0, \"russo\": 0, \"giapponese\": 0}\n",
    "for p in predictions: \n",
    "  max_predictions.append(max(p))\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for (i, p) in enumerate(predictions): \n",
    "  print(\"Predizione massima\", max_predictions[i], \n",
    "        \"Lingua: \", label[np.argmax(predictions[i])], \n",
    "        \"Lingua corretta: \", label[y_test_8[i]] + \"\\n\")\n",
    "  \n",
    "  valore = totali_lingua.get(label[np.argmax(predictions[i])])\n",
    "  valore += 1\n",
    "  totali_lingua[label[np.argmax(predictions[i])]] = valore\n",
    "\n",
    "  if (label[y_test_8[i]] == label[np.argmax(predictions[i])]): \n",
    "    valore1 = corrette_lingua.get(label[y_test_8[i]])\n",
    "    valore1 += 1\n",
    "    corrette_lingua[label[y_test_8[i]]] = valore1\n",
    "  \n",
    "print(\"Corrette per linguae: \", corrette_lingua)\n",
    "print(\"Totali per lingua: \", totali_lingua)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_gru_combined.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
